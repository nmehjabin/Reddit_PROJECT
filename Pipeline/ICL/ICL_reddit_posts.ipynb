{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5GGfhCJ3dmS",
        "outputId": "0fa92286-0275-40a0-d289-5b4ec339b6fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.12/dist-packages (0.77.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from anthropic import Anthropic\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load your data\n",
        "print(\"Loading data...\")\n",
        "big_data = pd.read_csv('posts_processed2.csv')\n",
        "labeled_data = pd.read_excel('manual_label_batch1.xlsx')\n",
        "\n",
        "print(f\" Big dataset: {len(big_data)} rows\")\n",
        "print(f\" Labeled dataset: {len(labeled_data)} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuetziMN3gc4",
        "outputId": "cfe3feb8-6c28-4773-ba16-11ad63aa61cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            " Big dataset: 18653 rows\n",
            " Labeled dataset: 177 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STRICT BINARY: Only label 1 is relevant\n",
        "labeled_data['binary_label'] = (labeled_data['label'] == 1).astype(int)\n",
        "\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(f\"Label 1 (In-scope): {(labeled_data['binary_label'] == 1).sum()} posts\")\n",
        "print(f\"All others (Out-of-scope): {(labeled_data['binary_label'] == 0).sum()} posts\")\n",
        "\n",
        "# Get examples\n",
        "n_examples_per_class = 8\n",
        "label_1_examples = labeled_data[labeled_data['binary_label'] == 1].sample(min(n_examples_per_class, (labeled_data['binary_label'] == 1).sum()), random_state=42)\n",
        "other_examples = labeled_data[labeled_data['binary_label'] == 0].sample(min(n_examples_per_class, (labeled_data['binary_label'] == 0).sum()), random_state=42)\n",
        "\n",
        "print(f\"\\nUsing {len(label_1_examples)} Label-1 and {len(other_examples)} other examples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqQqIbnm3mEv",
        "outputId": "b94b2b38-6eae-4726-d263-dbf4bf5caa46"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Label distribution:\n",
            "Label 1 (In-scope): 38 posts\n",
            "All others (Out-of-scope): 139 posts\n",
            "\n",
            "Using 8 Label-1 and 8 other examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create prompt\n",
        "def create_strict_binary_prompt(label_1_ex, other_ex):\n",
        "    prompt = \"\"\"You are an expert at identifying work-related burnout, chronic stress, and\n",
        "    workload mental pressure and burnout in Reddit posts from cybersecurity professionals.\n",
        "\n",
        "Your task is to classify posts as either:\n",
        "- Label 1 (IN-SCOPE): Posts SPECIFICALLY about work-related burnout, chronic stress from job demands,\n",
        "overwhelming workload, work pressure, long hours, on-call exhaustion, or feeling burnt out from work.\n",
        "- Label 0 (OUT-OF-SCOPE): Everything else including: imposter syndrome, general anxiety (not work-stress), PTSD,\n",
        "mental health disorders, job search issues, work-life balance tips,\n",
        "stress relief advice, technical questions, news, products, memes, etc.\n",
        "\n",
        "IMPORTANT: Be STRICT. Only classify as Label 1 if the post is clearly about work burnout,\n",
        "chronic work stress, or mental burnout and stress.\n",
        "\n",
        "Here are examples of Label 1 (IN-SCOPE) posts:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    for i, row in label_1_ex.iterrows():\n",
        "        prompt += f\"Example {i+1}:\\n\\\"{row['text'][:400]}...\\\"\\nLabel: 1\\n\\n\"\n",
        "\n",
        "    prompt += \"\\nHere are examples of Label 0 (OUT-OF-SCOPE) posts:\\n\\n\"\n",
        "\n",
        "    for i, row in other_ex.iterrows():\n",
        "        prompt += f\"Example {i+1}:\\n\\\"{row['text'][:400]}...\\\"\\nLabel: 0\\n\\n\"\n",
        "\n",
        "    prompt += \"\"\"\\nNow classify the following post. Respond with ONLY a JSON object:\n",
        "{\n",
        "  \"label\": 0 or 1,\n",
        "  \"confidence\": a number between 0.0 and 1.0,\n",
        "  \"reasoning\": \"brief explanation\"\n",
        "}\n",
        "\n",
        "Post to classify:\n",
        "\"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "XGL5iQIi3vUA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_prompt = create_strict_binary_prompt(label_1_examples, other_examples)\n",
        "client = Anthropic()\n",
        "\n",
        "# Test on small sample first\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING ON 10 SAMPLES:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_sample = big_data.sample(20, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU152OET4g12",
        "outputId": "e63d989f-c3a5-497b-9fa7-306379fe7010"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING ON 10 SAMPLES:\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_post_icl(text, base_prompt, max_retries=3):\n",
        "    full_prompt = base_prompt + f\"\\\"{text}\\\"\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.messages.create(\n",
        "                model=\"claude-sonnet-4-20250514\",\n",
        "                max_tokens=300,\n",
        "                temperature=0,\n",
        "                messages=[{\"role\": \"user\", \"content\": full_prompt}]\n",
        "            )\n",
        "\n",
        "            response_text = response.content[0].text.strip()\n",
        "\n",
        "            if '{' in response_text and '}' in response_text:\n",
        "                json_start = response_text.index('{')\n",
        "                json_end = response_text.rindex('}') + 1\n",
        "                json_str = response_text[json_start:json_end]\n",
        "                result = json.loads(json_str)\n",
        "\n",
        "                return {\n",
        "                    'label': result.get('label', -1),\n",
        "                    'confidence': result.get('confidence', 0.5),\n",
        "                    'reasoning': result.get('reasoning', '')\n",
        "                }\n",
        "            else:\n",
        "                return {'label': -1, 'confidence': 0.5, 'reasoning': 'Parse error'}\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                return {'label': -1, 'confidence': 0.5, 'reasoning': f'Error: {str(e)}'}\n",
        "\n",
        "for idx, row in test_sample.iterrows():\n",
        "    print(f\"\\n--- Sample {idx} ---\")\n",
        "    print(f\"Text: {row['cleaned_text'][:150]}...\")\n",
        "    result = classify_post_icl(row['cleaned_text'], base_prompt)\n",
        "    print(f\"Label: {result['label']} | Confidence: {result['confidence']:.2f}\")\n",
        "    print(f\"Reasoning: {result['reasoning']}\")\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Proceed with full dataset classification? (type 'yes' to continue)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP53lW3h4wqr",
        "outputId": "43c9b779-3cb6-4336-b92e-8b65e08844a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 16852 ---\n",
            "Text: what are the best data breach reports to compare to the verizon dbir i ve wondered for a while now i ve been reading the vdbir almost yearly and only ...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 17679 ---\n",
            "Text: cybersecurity education survey...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 2146 ---\n",
            "Text: what are aspects of cybersecurity most people hate but are super important hi when i started my cybersecurity career i started out as a penetration te...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 17979 ---\n",
            "Text: i negotiated with ransomware actors. ask me anything. hello everyone. for this ama the editors at ciso series assembled a handful of ransomware negoti...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 3846 ---\n",
            "Text: cloud vs technical security....which is better for job security retired military    years.    years it security . cissp and other bullshit. jobless fo...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 7289 ---\n",
            "Text: how does a processor execute encrypted binaries. i have most of my education within software development but i have been reading up on red teaming ope...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 1087 ---\n",
            "Text: does anyone know of a  rd party vendor risk monitoring or verification tool hi guys does anyone here use a tool that actually checks for things like m...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 15232 ---\n",
            "Text: having trouble understanding sso oauth openid connect saml and jwt and how some of them work together. hello guys i am having a hard time understandin...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 14986 ---\n",
            "Text: what was the worst case of shadow it or the biggest problem caused by shadow it you ve encountered during your work basically the title....\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 14558 ---\n",
            "Text: american water warns of billing outages after finding hackers in its systems...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 4554 ---\n",
            "Text: hypothetically: if an intelligence program could access enough to create the illusion of being able to access nearly every electronic thing and or nea...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 15975 ---\n",
            "Text: what s your take on automatic security updates especially with multiple trusted sources for a new wordpress fork   subreddit we re creating a new plug...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 42 ---\n",
            "Text: pnls: tool capable of capturing ssids from device s preferred network list in the nearby vicinity hi everyone i was tinkering with this idea for a whi...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 15333 ---\n",
            "Text: fqdn filtering in cloud security: a technical deep dive i have written a summary of fqdn filtering approaches using some cloud firewalls to illustrate...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 16854 ---\n",
            "Text: has anyone worked in a family business and it part time just want to know since i have yet to get a job but now i was told to manage a family business...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 1989 ---\n",
            "Text: put all checked api methods on the allow list and any new api methods that might be introduces by a cloud provider into their service to be blocked un...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 9767 ---\n",
            "Text: job scams surged     in      due to ai watchdog group warns...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 14640 ---\n",
            "Text: burnt out soc analyst where to go from here hello infosec community i am a burnt out sr. threat analyst that has been working in the soc for the past ...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 10184 ---\n",
            "Text: trying to move to a third party risk role good afternoon i m interviewing for a third party risk role rather soon and i m looking for some resources t...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "--- Sample 10957 ---\n",
            "Text: vulnerability summary for the week of july         cisa...\n",
            "Label: -1 | Confidence: 0.50\n",
            "Reasoning: Error: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
            "\n",
            "============================================================\n",
            "Proceed with full dataset classification? (type 'yes' to continue)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Classify the FULL dataset\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFYING FULL DATASET:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total posts to classify: {len(big_data)}\")\n",
        "print(\"Estimated time: ~5 hours (1 request per second)\")\n",
        "print(\"Progress will be saved every 500 posts\")\n",
        "\n",
        "# Create a results directory if it doesn't exist\n",
        "os.makedirs('icl_results', exist_ok=True)\n",
        "\n",
        "# Check if we have any saved progress\n",
        "progress_files = [f for f in os.listdir('icl_results') if f.startswith('progress_')]\n",
        "start_index = 0\n",
        "\n",
        "if progress_files:\n",
        "    # Find the latest progress file\n",
        "    latest_progress = max(progress_files)\n",
        "    print(f\"\\nFound existing progress file: {latest_progress}\")\n",
        "\n",
        "    # Load existing results\n",
        "    existing_results = pd.read_csv(f'icl_results/{latest_progress}')\n",
        "    results = existing_results.to_dict('records')\n",
        "    start_index = len(results)\n",
        "\n",
        "    print(f\"Resuming from index {start_index}\")\n",
        "else:\n",
        "    results = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5VRHNQN7Rtr",
        "outputId": "0e7459e5-61d4-4f61-ee44-55460d8e2934"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CLASSIFYING FULL DATASET:\n",
            "============================================================\n",
            "Total posts to classify: 18653\n",
            "Estimated time: ~5 hours (1 request per second)\n",
            "Progress will be saved every 500 posts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all posts with progress bar\n",
        "batch_size = 100  # Process in batches for easier management\n",
        "save_interval = 500  # Save progress every 500 posts\n",
        "\n",
        "print(f\"\\nStarting classification from index {start_index}...\")\n",
        "\n",
        "for i in tqdm(range(start_index, len(big_data)), desc=\"Classifying posts\"):\n",
        "    row = big_data.iloc[i]\n",
        "\n",
        "    # Classify the post\n",
        "    result = classify_post_icl(row['cleaned_text'], base_prompt)\n",
        "\n",
        "    # Store the result\n",
        "    results.append({\n",
        "        'index': i,\n",
        "        'label': result['label'],\n",
        "        'confidence': result['confidence'],\n",
        "        'reasoning': result['reasoning']\n",
        "    })\n",
        "\n",
        "    # Rate limiting - wait 1 second between API calls\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Save progress periodically\n",
        "    if (i + 1) % save_interval == 0:\n",
        "        temp_results_df = pd.DataFrame(results)\n",
        "        progress_file = f'icl_results/progress_batch_{i+1}.csv'\n",
        "        temp_results_df.to_csv(progress_file, index=False)\n",
        "        print(f\"\\n Progress saved at {i+1} posts to {progress_file}\")\n",
        "\n",
        "print(\"\\n Classification complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hy7YU537gmI",
        "outputId": "3433beec-51f9-4c13-9432-b3928cf3e681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting classification from index 0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Classifying posts:   0%|          | 28/18653 [02:20<25:53:32,  5.00s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to dataframe\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Merge results with original data\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MERGING RESULTS WITH ORIGINAL DATA:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "big_data['icl_label'] = results_df['label'].values\n",
        "big_data['icl_confidence'] = results_df['confidence'].values\n",
        "big_data['icl_reasoning'] = results_df['reasoning'].values"
      ],
      "metadata": {
        "id": "7eFmbr9M7rKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate summary statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"IN-CONTEXT LEARNING RESULTS:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_label_1 = (big_data['icl_label'] == 1).sum()\n",
        "total_others = (big_data['icl_label'] == 0).sum()\n",
        "total_errors = (big_data['icl_label'] == -1).sum()\n",
        "\n",
        "print(f\"Label 1 (Work burnout/stress/workload): {total_label_1} posts ({total_label_1/len(big_data)*100:.1f}%)\")\n",
        "print(f\"Label 0 (All others - Out-of-scope): {total_others} posts ({total_others/len(big_data)*100:.1f}%)\")\n",
        "print(f\"Classification errors: {total_errors} posts ({total_errors/len(big_data)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nAverage confidence score: {big_data['icl_confidence'].mean():.3f}\")"
      ],
      "metadata": {
        "id": "YVopw2TO7wR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confidence distribution\n",
        "print(\"\\nConfidence distribution:\")\n",
        "for threshold in [0.6, 0.7, 0.8, 0.9]:\n",
        "    high_conf = (big_data['icl_confidence'] >= threshold).sum()\n",
        "    print(f\"  Predictions with ≥{threshold:.1f} confidence: {high_conf} ({high_conf/len(big_data)*100:.1f}%)\")\n",
        "\n",
        "# Save all results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING FINAL RESULTS:\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "_mS5y8wv7z6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confidence distribution\n",
        "print(\"\\nConfidence distribution:\")\n",
        "for threshold in [0.6, 0.7, 0.8, 0.9]:\n",
        "    high_conf = (big_data['icl_confidence'] >= threshold).sum()\n",
        "    print(f\"  Predictions with ≥{threshold:.1f} confidence: {high_conf} ({high_conf/len(big_data)*100:.1f}%)\")\n",
        "\n",
        "# Save all results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING FINAL RESULTS:\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "JyR3qaMb75Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Only Label 1 posts (work burnout/stress/workload)\n",
        "label_1_posts = big_data[big_data['icl_label'] == 1]\n",
        "label_1_output = \"reddit_LABEL1_ICL_only.csv\"\n",
        "label_1_posts.to_csv(label_1_output, index=False)\n",
        "print(f\" Label 1 posts saved to: {label_1_output}\")\n",
        "print(f\"  Total: {len(label_1_posts)} posts\")"
      ],
      "metadata": {
        "id": "b0u5ivA779hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. High confidence Label 1 posts (confidence > 0.7)\n",
        "high_conf_label_1 = big_data[\n",
        "    (big_data['icl_label'] == 1) &\n",
        "    (big_data['icl_confidence'] > 0.7)\n",
        "]\n",
        "high_conf_output = \"reddit_LABEL1_ICL_high_confidence.csv\"\n",
        "high_conf_label_1.to_csv(high_conf_output, index=False)\n",
        "print(f\" High confidence Label 1 saved to: {high_conf_output}\")\n",
        "print(f\"  Total: {len(high_conf_label_1)} posts\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_MODVrkk7-NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Very high confidence Label 1 posts (confidence > 0.85)\n",
        "very_high_conf = big_data[\n",
        "    (big_data['icl_label'] == 1) &\n",
        "    (big_data['icl_confidence'] > 0.85)\n",
        "]\n",
        "very_high_output = \"reddit_LABEL1_ICL_very_high_confidence.csv\"\n",
        "very_high_conf.to_csv(very_high_output, index=False)\n",
        "print(f\" Very high confidence Label 1 saved to: {very_high_output}\")\n",
        "print(f\"  Total: {len(very_high_conf)} posts\")\n"
      ],
      "metadata": {
        "id": "P9w4ULbR8F9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Uncertain predictions (confidence between 0.4 and 0.6)\n",
        "uncertain = big_data[\n",
        "    (big_data['icl_confidence'] > 0.4) &\n",
        "    (big_data['icl_confidence'] < 0.6)\n",
        "]\n",
        "uncertain_output = \"reddit_ICL_uncertain_predictions.csv\"\n",
        "uncertain.to_csv(uncertain_output, index=False)\n",
        "print(f\" Uncertain predictions saved to: {uncertain_output}\")\n",
        "print(f\"  Total: {len(uncertain)} posts (recommend manual review)\")\n"
      ],
      "metadata": {
        "id": "swav68kV8HT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Uncertain predictions (confidence between 0.4 and 0.6)\n",
        "uncertain = big_data[\n",
        "    (big_data['icl_confidence'] > 0.4) &\n",
        "    (big_data['icl_confidence'] < 0.6)\n",
        "]\n",
        "uncertain_output = \"reddit_ICL_uncertain_predictions.csv\"\n",
        "uncertain.to_csv(uncertain_output, index=False)\n",
        "print(f\"✓ Uncertain predictions saved to: {uncertain_output}\")\n",
        "print(f\"  Total: {len(uncertain)} posts (recommend manual review)\")\n"
      ],
      "metadata": {
        "id": "3EZh-Kuw8T57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Posts with classification errors (label = -1)\n",
        "if total_errors > 0:\n",
        "    error_posts = big_data[big_data['icl_label'] == -1]\n",
        "    error_output = \"reddit_ICL_classification_errors.csv\"\n",
        "    error_posts.to_csv(error_output, index=False)\n",
        "    print(f\"✓ Classification errors saved to: {error_output}\")\n",
        "    print(f\"  Total: {len(error_posts)} posts (need reprocessing)\")\n"
      ],
      "metadata": {
        "id": "y-HXB5Lx8UvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Sample review file - stratified by confidence levels\n",
        "print(\"\\nCreating stratified sample for quality review...\")\n"
      ],
      "metadata": {
        "id": "wd76Hp-G8W7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_parts = []\n",
        "\n",
        "# High confidence Label 1\n",
        "if len(high_conf_label_1) > 0:\n",
        "    sample_parts.append(high_conf_label_1.sample(min(30, len(high_conf_label_1)), random_state=42))\n",
        "\n",
        "# Low confidence Label 1\n",
        "low_conf_label_1 = label_1_posts[label_1_posts['icl_confidence'] <= 0.7]\n",
        "if len(low_conf_label_1) > 0:\n",
        "    sample_parts.append(low_conf_label_1.sample(min(20, len(low_conf_label_1)), random_state=42))\n",
        "\n",
        "# Label 0 (others)\n",
        "label_0_posts = big_data[big_data['icl_label'] == 0]\n",
        "if len(label_0_posts) > 0:\n",
        "    sample_parts.append(label_0_posts.sample(min(30, len(label_0_posts)), random_state=42))\n",
        "\n",
        "# Uncertain predictions\n",
        "if len(uncertain) > 0:\n",
        "    sample_parts.append(uncertain.sample(min(20, len(uncertain)), random_state=42))\n",
        "\n",
        "# Combine samples\n",
        "sample_review = pd.concat(sample_parts, ignore_index=True)\n",
        "sample_review.to_csv(\"reddit_ICL_sample_for_review.csv\", index=False)\n",
        "print(f\" Sample review file saved: reddit_ICL_sample_for_review.csv\")\n",
        "print(f\"  Contains {len(sample_review)} samples for quality checking\")"
      ],
      "metadata": {
        "id": "aAn56HGr8Zd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a summary report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING SUMMARY REPORT:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary_report = {\n",
        "    'Total Posts': len(big_data),\n",
        "    'Label 1 (Burnout)': total_label_1,\n",
        "    'Label 1 Percentage': f\"{total_label_1/len(big_data)*100:.2f}%\",\n",
        "    'Label 0 (Others)': total_others,\n",
        "    'Label 0 Percentage': f\"{total_others/len(big_data)*100:.2f}%\",\n",
        "    'Classification Errors': total_errors,\n",
        "    'Average Confidence': f\"{big_data['icl_confidence'].mean():.3f}\",\n",
        "    'High Confidence (>0.7) Posts': (big_data['icl_confidence'] > 0.7).sum(),\n",
        "    'Very High Confidence (>0.85) Posts': (big_data['icl_confidence'] > 0.85).sum(),\n",
        "    'Uncertain (0.4-0.6) Posts': len(uncertain)\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame([summary_report]).T\n",
        "summary_df.columns = ['Value']\n",
        "summary_df.to_csv(\"reddit_ICL_summary_report.csv\")\n",
        "print(f\" Summary report saved to: reddit_ICL_summary_report.csv\")\n",
        "print(\"\\nSummary:\")\n",
        "print(summary_df)\n"
      ],
      "metadata": {
        "id": "lo0LUWW08qC6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}