{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b932a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SEMANTIC SIMILARITY SEARCH - CSV VERSION\n",
      "====================================================================================================\n",
      "\n",
      "Loading model: all-MiniLM-L6-v2...\n",
      "✓ Model loaded successfully!\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No data loaded! Call load_csv() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 337\u001b[39m\n\u001b[32m    323\u001b[39m text_col = \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m       \u001b[38;5;66;03m# Name of column with text to search\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# Uncomment and modify as needed:\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[38;5;66;03m# searcher.load_csv(csv_file, text_column=text_col)\u001b[39;00m\n\u001b[32m    327\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# STEP 3: Encode the data (do this ONCE!)\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m \u001b[43msearcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# STEP 4: Search for burnout-related content\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# STEP 5: Multi-query search (RECOMMENDED - better results!)\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[32m    355\u001b[39m burnout_queries = [\n\u001b[32m    356\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mburnout\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    357\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexhaustion\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m    362\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmental health work\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    363\u001b[39m ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mCSVSemanticSearcher.encode_data\u001b[39m\u001b[34m(self, batch_size, show_progress)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03mEncode all texts for semantic search.\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[33;03mThis step takes time but only needs to be done ONCE!\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m \u001b[33;03m    show_progress: Show progress bar\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo data loaded! Call load_csv() first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m texts = \u001b[38;5;28mself\u001b[39m.df[\u001b[38;5;28mself\u001b[39m.text_column].tolist()\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEncoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: No data loaded! Call load_csv() first."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Semantic Similarity Search - CSV Data Version\n",
    "Load your CSV file and search for burnout-related content\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CSVSemanticSearcher:\n",
    "    \"\"\"\n",
    "    Semantic search tool that works directly with CSV files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the semantic searcher.\n",
    "        \n",
    "        Args:\n",
    "            model_name: \n",
    "                - 'all-MiniLM-L6-v2' (RECOMMENDED - fast, balanced)\n",
    "                - 'all-mpnet-base-v2' (more accurate, slower)\n",
    "                - 'multi-qa-MiniLM-L6-cos-v1' (optimized for search)\n",
    "        \"\"\"\n",
    "        print(f\"Loading model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        print(\"✓ Model loaded successfully!\\n\")\n",
    "        \n",
    "        self.df = None\n",
    "        self.embeddings = None\n",
    "        self.text_column = None\n",
    "        \n",
    "    def load_csv(self, \n",
    "                 csv_path: str, \n",
    "                 text_column: str,\n",
    "                 encoding: str = 'utf-8',\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Load CSV file and prepare for search.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to your CSV file\n",
    "            text_column: Name of the column containing text to search\n",
    "            encoding: File encoding (default 'utf-8', try 'latin-1' if error)\n",
    "            **kwargs: Additional arguments for pd.read_csv()\n",
    "        \n",
    "        Example:\n",
    "            searcher.load_csv('data.csv', text_column='comments')\n",
    "            searcher.load_csv('data.csv', text_column='feedback', encoding='latin-1')\n",
    "        \"\"\"\n",
    "        print(f\"Loading CSV: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            self.df = pd.read_csv(csv_path, encoding=encoding, **kwargs)\n",
    "            print(f\"\\ Loaded {len(self.df):,} rows\")\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\" Encoding error with '{encoding}', trying 'latin-1'...\")\n",
    "            self.df = pd.read_csv(csv_path, encoding='latin-1', **kwargs)\n",
    "            print(f\" Loaded {len(self.df):,} rows\")\n",
    "        \n",
    "        # Validate column exists\n",
    "        if text_column not in self.df.columns:\n",
    "            available_cols = ', '.join(self.df.columns.tolist())\n",
    "            raise ValueError(\n",
    "                f\"Column '{text_column}' not found!\\n\"\n",
    "                f\"Available columns: {available_cols}\"\n",
    "            )\n",
    "        \n",
    "        self.text_column = text_column\n",
    "        \n",
    "        # Handle missing values\n",
    "        missing_count = self.df[text_column].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\" Found {missing_count} missing values - filling with empty strings\")\n",
    "            self.df[text_column] = self.df[text_column].fillna('')\n",
    "        \n",
    "        # Convert to string\n",
    "        self.df[text_column] = self.df[text_column].astype(str)\n",
    "        \n",
    "        print(f\" Using column: '{text_column}'\")\n",
    "        print(f\"\\nSample data:\")\n",
    "        for i, text in enumerate(self.df[text_column].head(3)):\n",
    "            preview = text[:100] + \"...\" if len(text) > 100 else text\n",
    "            print(f\"  Row {i+1}: {preview}\")\n",
    "        print()\n",
    "        \n",
    "    def encode_data(self, batch_size: int = 32, show_progress: bool = True):\n",
    "        \"\"\"\n",
    "        Encode all texts for semantic search.\n",
    "        This step takes time but only needs to be done ONCE!\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of texts to encode at once (increase if you have GPU)\n",
    "            show_progress: Show progress bar\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"No data loaded! Call load_csv() first.\")\n",
    "        \n",
    "        texts = self.df[self.text_column].tolist()\n",
    "        \n",
    "        print(f\"Encoding {len(texts):,} documents...\")\n",
    "        print(\"(This may take a few minutes - grab a coffee! ☕)\")\n",
    "        \n",
    "        self.embeddings = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=show_progress\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Encoding complete!\\n\")\n",
    "        \n",
    "    def search(self, \n",
    "               query: str,\n",
    "               top_k: int = 100,\n",
    "               min_score: float = 0.30,\n",
    "               return_full_data: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Search for semantically similar content.\n",
    "        \n",
    "        Args:\n",
    "            query: Search term (e.g., \"burnout\")\n",
    "            top_k: Maximum number of results\n",
    "            min_score: Minimum similarity score (0-1)\n",
    "            return_full_data: If True, returns all columns from original CSV\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with search results\n",
    "        \"\"\"\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"Data not encoded! Call encode_data() first.\")\n",
    "        \n",
    "        print(f\" Searching for: '{query}'\")\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = util.cos_sim(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top results\n",
    "        top_results = similarities.topk(k=min(top_k, len(self.df)))\n",
    "        \n",
    "        # Build results\n",
    "        indices = []\n",
    "        scores = []\n",
    "        \n",
    "        for score, idx in zip(top_results.values, top_results.indices):\n",
    "            score_val = score.item()\n",
    "            if score_val >= min_score:\n",
    "                indices.append(idx.item())\n",
    "                scores.append(score_val)\n",
    "        \n",
    "        if len(indices) == 0:\n",
    "            print(f\"No results found with score >= {min_score}\\n\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Create results dataframe\n",
    "        if return_full_data:\n",
    "            # Return all original columns plus score\n",
    "            results_df = self.df.iloc[indices].copy()\n",
    "        else:\n",
    "            # Return only text column\n",
    "            results_df = pd.DataFrame({\n",
    "                self.text_column: self.df.iloc[indices][self.text_column].values\n",
    "            })\n",
    "        \n",
    "        results_df['similarity_score'] = scores\n",
    "        results_df['search_rank'] = range(1, len(results_df) + 1)\n",
    "        \n",
    "        # Reorder columns to put score and rank first\n",
    "        cols = ['search_rank', 'similarity_score'] + [c for c in results_df.columns \n",
    "                                                        if c not in ['search_rank', 'similarity_score']]\n",
    "        results_df = results_df[cols]\n",
    "        \n",
    "        print(f\" Found {len(results_df)} results\")\n",
    "        print(f\"  Score range: {results_df['similarity_score'].min():.3f} - {results_df['similarity_score'].max():.3f}\\n\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def multi_search(self,\n",
    "                    queries: List[str],\n",
    "                    top_k: int = 100,\n",
    "                    min_score: float = 0.25,\n",
    "                    combine_method: str = 'max',\n",
    "                    return_full_data: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Search using multiple related queries (RECOMMENDED for better results).\n",
    "        \n",
    "        Args:\n",
    "            queries: List of related search terms\n",
    "            top_k: Max results per query\n",
    "            min_score: Minimum similarity score\n",
    "            combine_method: 'max' (take highest score) or 'average'\n",
    "            return_full_data: Return all original columns\n",
    "        \n",
    "        Returns:\n",
    "            Combined and deduplicated results\n",
    "        \"\"\"\n",
    "        print(f\" Multi-query search ({len(queries)} queries):\")\n",
    "        for q in queries:\n",
    "            print(f\"   • {q}\")\n",
    "        print()\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for query in queries:\n",
    "            df = self.search(query, top_k=top_k, min_score=min_score, \n",
    "                           return_full_data=return_full_data)\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                if idx not in all_results:\n",
    "                    all_results[idx] = {\n",
    "                        'data': row.to_dict(),\n",
    "                        'scores': [row['similarity_score']],\n",
    "                        'queries': [query]\n",
    "                    }\n",
    "                else:\n",
    "                    all_results[idx]['scores'].append(row['similarity_score'])\n",
    "                    all_results[idx]['queries'].append(query)\n",
    "        \n",
    "        if len(all_results) == 0:\n",
    "            print(\"✗ No results found across all queries\\n\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Combine results\n",
    "        combined = []\n",
    "        for idx, data in all_results.items():\n",
    "            row = data['data'].copy()\n",
    "            \n",
    "            # Calculate combined score\n",
    "            if combine_method == 'max':\n",
    "                row['similarity_score'] = max(data['scores'])\n",
    "            else:\n",
    "                row['similarity_score'] = np.mean(data['scores'])\n",
    "            \n",
    "            row['num_queries_matched'] = len(data['queries'])\n",
    "            row['matched_queries'] = ', '.join(data['queries'])\n",
    "            \n",
    "            combined.append(row)\n",
    "        \n",
    "        results_df = pd.DataFrame(combined)\n",
    "        results_df = results_df.sort_values('similarity_score', ascending=False).reset_index(drop=True)\n",
    "        results_df['search_rank'] = range(1, len(results_df) + 1)\n",
    "        \n",
    "        # Reorder columns\n",
    "        priority_cols = ['search_rank', 'similarity_score', 'num_queries_matched', 'matched_queries']\n",
    "        other_cols = [c for c in results_df.columns if c not in priority_cols]\n",
    "        results_df = results_df[priority_cols + other_cols]\n",
    "        \n",
    "        print(f\"✓ Combined {len(results_df)} unique results\")\n",
    "        print(f\"  Score range: {results_df['similarity_score'].min():.3f} - {results_df['similarity_score'].max():.3f}\\n\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def show_summary(self, results_df: pd.DataFrame, n: int = 10):\n",
    "        \"\"\"\n",
    "        Display a nice summary of search results.\n",
    "        \n",
    "        Args:\n",
    "            results_df: Results from search() or multi_search()\n",
    "            n: Number of results to show\n",
    "        \"\"\"\n",
    "        if len(results_df) == 0:\n",
    "            print(\"No results to display.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 100)\n",
    "        print(f\"TOP {min(n, len(results_df))} RESULTS\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        for i, row in results_df.head(n).iterrows():\n",
    "            print(f\"\\n[Rank {row['search_rank']}] Score: {row['similarity_score']:.3f}\", end='')\n",
    "            \n",
    "            if 'num_queries_matched' in row:\n",
    "                print(f\" | Matched {row['num_queries_matched']} queries\")\n",
    "            else:\n",
    "                print()\n",
    "            \n",
    "            # Show text preview\n",
    "            text = str(row[self.text_column])\n",
    "            preview = text[:200] + \"...\" if len(text) > 200 else text\n",
    "            print(f\"   {preview}\")\n",
    "            \n",
    "            # Show other relevant columns (first 3 non-score columns)\n",
    "            display_cols = [c for c in row.index \n",
    "                          if c not in [self.text_column, 'similarity_score', \n",
    "                                     'search_rank', 'num_queries_matched', 'matched_queries']][:3]\n",
    "            if display_cols:\n",
    "                print(f\"   Additional data: \", end='')\n",
    "                info = [f\"{col}={row[col]}\" for col in display_cols]\n",
    "                print(\", \".join(info))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE WITH YOUR CSV\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"SEMANTIC SIMILARITY SEARCH - CSV VERSION\")\n",
    "    print(\"=\" * 100)\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Initialize searcher\n",
    "    # ========================================================================\n",
    "    searcher = CSVSemanticSearcher(model_name='all-MiniLM-L6-v2')\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Load your CSV file\n",
    "    # ========================================================================\n",
    "    # CHANGE THESE VALUES TO MATCH YOUR DATA:\n",
    "    searcher.load_csv(\n",
    "        csv_path='posts_processed2.csv',      # ← Your CSV file path\n",
    "        text_column='text'  # ← Your text column name\n",
    "    )\n",
    "\n",
    "\n",
    "    # Uncomment and modify as needed:\n",
    "    # searcher.load_csv(csv_file, text_column=text_col)\n",
    "    \n",
    "    # If your file has encoding issues, try:\n",
    "    # searcher.load_csv(csv_file, text_column=text_col, encoding='latin-1')\n",
    "    \n",
    "    # If you need to specify delimiter or other options:\n",
    "    # searcher.load_csv(csv_file, text_column=text_col, sep=';', encoding='utf-8')\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Encode the data (do this ONCE!)\n",
    "    # ========================================================================\n",
    "    searcher.encode_data()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: Search for burnout-related content\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Single query search\n",
    "    # results = searcher.search(\n",
    "    #     query=\"burnout\",\n",
    "    #     top_k=100,\n",
    "    #     min_score=0.30\n",
    "    # )\n",
    "    # searcher.show_summary(results, n=10)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: Multi-query search (RECOMMENDED - better results!)\n",
    "    # ========================================================================\n",
    "    \n",
    "    burnout_queries = [\n",
    "        \"burnout\",\n",
    "        \"exhaustion\", \n",
    "        \"work stress\",\n",
    "        \"overwhelmed\",\n",
    "        \"fatigue\",\n",
    "        \"drained\",\n",
    "        \"mental health work\"\n",
    "    ]\n",
    "    \n",
    "    results = searcher.multi_search(\n",
    "        queries=burnout_queries,\n",
    "        top_k=100,\n",
    "        min_score=0.25\n",
    "    )\n",
    "    \n",
    "    searcher.show_summary(results, n=20)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: Save results\n",
    "    # ========================================================================\n",
    "    \n",
    "    results.to_csv('burnout_search_results.csv', index=False)\n",
    "    print(\"\\n Results saved to: burnout_search_results.csv\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 7: Analyze results\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"ANALYSIS\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"Total results found: {len(results)}\")\n",
    "    print(f\"Score distribution:\")\n",
    "    print(f\"  > 0.60 (high confidence): {len(results[results['similarity_score'] > 0.60])}\")\n",
    "    print(f\"  0.40-0.60 (medium):       {len(results[(results['similarity_score'] >= 0.40) & (results['similarity_score'] <= 0.60)])}\")\n",
    "    print(f\"  < 0.40 (low):             {len(results[results['similarity_score'] < 0.40])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"INSTRUCTIONS:\")\n",
    "    print(\"=\"*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
