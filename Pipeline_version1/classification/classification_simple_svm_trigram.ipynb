{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Text Classification Model Training - Simple SVM with Trigrams\n",
    "Steps \n",
    "1. Data Loading & Exploration\n",
    "2. Data Preprocessing\n",
    "3. Handling Multi-labels\n",
    "4. Train-Test Split\n",
    "5. Feature Extraction (TF-IDF with Trigrams)\n",
    "6. Model Training (SVM)\n",
    "7. Model Evaluation\n",
    "8. Model Saving\n",
    "9. Prediction on New Data\n",
    "\n",
    "Libraries \n",
    "pip install scikit-learn pandas numpy matplotlib seaborn openpyxl xlrd --break-system-packages\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d94352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary libraries are imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             accuracy_score, f1_score)\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All necessary libraries are imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a7dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: DATA LOADING & EXPLORATION\n",
    "\n",
    "def load_and_explore_data(file_path):\n",
    "    \"\"\"Load data and perform initial exploration\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 1: DATA LOADING & EXPLORATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    print(f\"\\nText length statistics:\")\n",
    "    df['text_length'] = df['text'].apply(lambda x: len(str(x)))\n",
    "    print(df['text_length'].describe())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdfc1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: DATA PREPROCESSING\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e93b19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: HANDLING MULTI-LABELS\n",
    "\n",
    "def handle_multilabels(df, strategy='first'):\n",
    "    \"\"\"\n",
    "    Handle multi-label samples\n",
    "    \n",
    "    Strategies:\n",
    "    - 'first': Take the first label\n",
    "    - 'remove': Remove multi-label samples\n",
    "    - 'separate': Create separate samples for each label\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: HANDLING MULTI-LABELS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Identify multi-label samples\n",
    "    multi_label_mask = df['label'].astype(str).str.contains(',')\n",
    "    multi_label_count = multi_label_mask.sum()\n",
    "    \n",
    "    print(f\"\\nMulti-label samples found: {multi_label_count}\")\n",
    "    print(f\"Multi-label samples: {df[multi_label_mask]['label'].unique()}\")\n",
    "    \n",
    "    if strategy == 'first':\n",
    "        print(\"\\nStrategy: Taking first label from multi-label samples\")\n",
    "        df['label'] = df['label'].astype(str).apply(lambda x: x.split(',')[0])\n",
    "        \n",
    "    elif strategy == 'remove':\n",
    "        print(\"\\nStrategy: Removing multi-label samples\")\n",
    "        df = df[~multi_label_mask].copy()\n",
    "        \n",
    "    elif strategy == 'separate':\n",
    "        print(\"\\nStrategy: Creating separate samples for each label\")\n",
    "        new_rows = []\n",
    "        for idx, row in df[multi_label_mask].iterrows():\n",
    "            labels = str(row['label']).split(',')\n",
    "            for label in labels:\n",
    "                new_row = row.copy()\n",
    "                new_row['label'] = label.strip()\n",
    "                new_rows.append(new_row)\n",
    "        \n",
    "        # Remove original multi-label rows and add new rows\n",
    "        df = pd.concat([df[~multi_label_mask], pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    \n",
    "    # Convert labels to integers\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    \n",
    "    print(f\"\\nFinal label distribution:\")\n",
    "    print(df['label'].value_counts().sort_index())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b3b4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: TRAIN-TEST SPLIT\n",
    "\n",
    "def split_data(df, test_size=0.2, random_state=42, stratify=True):\n",
    "    \"\"\"Split data into train and test sets\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 4: TRAIN-TEST SPLIT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    X = df['text'].values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    # Use stratified split to maintain class distribution\n",
    "    if stratify:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    print(f\"\\nTraining set label distribution:\")\n",
    "    print(pd.Series(y_train).value_counts().sort_index())\n",
    "    print(f\"\\nTest set label distribution:\")\n",
    "    print(pd.Series(y_test).value_counts().sort_index())\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c61d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: FEATURE EXTRACTION (TF-IDF with Trigrams)\n",
    "\n",
    "def extract_features_tfidf_trigrams(X_train, X_test, max_features=1000):\n",
    "    \"\"\"Extract TF-IDF features with trigrams\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 5: FEATURE EXTRACTION (TF-IDF with Trigrams)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Use trigrams (1-3) instead of bigrams\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "        max_df=0.8,  # Ignore terms that appear in more than 80% of documents\n",
    "        ngram_range=(1, 3),  # Use unigrams, bigrams, and trigrams\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nTF-IDF matrix shape (train): {X_train_tfidf.shape}\")\n",
    "    print(f\"TF-IDF matrix shape (test): {X_test_tfidf.shape}\")\n",
    "    print(f\"Number of features: {len(vectorizer.get_feature_names_out())}\")\n",
    "    print(f\"\\nSample trigram features:\")\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    # Show some trigram examples\n",
    "    trigrams = [f for f in features if len(f.split()) == 3]\n",
    "    print(f\"Example trigrams: {trigrams[:10]}\")\n",
    "    \n",
    "    return X_train_tfidf, X_test_tfidf, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2924fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: MODEL TRAINING (SVM only)\n",
    "\n",
    "def train_svm_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train SVM model\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 6: MODEL TRAINING (SVM)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nTraining SVM with linear kernel...\")\n",
    "    \n",
    "    # Train SVM\n",
    "    model = SVC(kernel='linear', random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return model, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f12082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: MODEL EVALUATION\n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    \"\"\"Evaluate and visualize the model\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 7: MODEL EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - SVM with Trigrams')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('confusion_matrix_svm_trigram.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\nConfusion matrix saved to: confusion_matrix_svm_trigram.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e90eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: MODEL SAVING\n",
    "\n",
    "def save_model(model, vectorizer, filepath_model, filepath_vectorizer):\n",
    "    \"\"\"Save trained model and vectorizer\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 8: SAVING MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    # Save model\n",
    "    with open(filepath_model, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"\\nModel saved to: {filepath_model}\")\n",
    "    \n",
    "    # Save vectorizer\n",
    "    with open(filepath_vectorizer, 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    print(f\"Vectorizer saved to: {filepath_vectorizer}\")\n",
    "\n",
    "\n",
    "def load_model(filepath_model, filepath_vectorizer):\n",
    "    \"\"\"Load saved model and vectorizer\"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    with open(filepath_model, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    with open(filepath_vectorizer, 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    \n",
    "    return model, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb477395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9: PREDICTION ON NEW DATA\n",
    "\n",
    "def predict_new_data(model, vectorizer, new_texts):\n",
    "    \"\"\"Predict labels for new text data\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 9: PREDICTION ON NEW DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Preprocess\n",
    "    new_texts_processed = [preprocess_text(text) for text in new_texts]\n",
    "    \n",
    "    # Vectorize\n",
    "    new_texts_tfidf = vectorizer.transform(new_texts_processed)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(new_texts_tfidf)\n",
    "    \n",
    "    for i, (text, pred) in enumerate(zip(new_texts, predictions)):\n",
    "        print(f\"\\nText {i+1}: {text[:100]}...\")\n",
    "        print(f\"Predicted Label: {pred}\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5818bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEXT CLASSIFICATION: SVM WITH TRIGRAMS\n",
      "================================================================================\n",
      "================================================================================\n",
      "STEP 1: DATA LOADING & EXPLORATION\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (177, 4)\n",
      "\n",
      "Columns: ['id', 'text', 'similarity_score', 'label']\n",
      "\n",
      "First few rows:\n",
      "        id                                               text  \\\n",
      "0  1g4a7ot  Burn out among Cybersecurity leaders at a frus...   \n",
      "1  1dqiog2  Invitation to Participate in Research Study on...   \n",
      "2  1g49xt4  Dealing with feeling stuck in the security fie...   \n",
      "3  1fqxn7a  How are you doing guys?\\nIs this cybersecurity...   \n",
      "4  1fbdhwo  Hey folks, for those of you working right now,...   \n",
      "\n",
      "   similarity_score label  \n",
      "0          0.437536     1  \n",
      "1          0.400815     0  \n",
      "2          0.281245     0  \n",
      "3          0.316323     1  \n",
      "4          0.377892     2  \n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0      107\n",
      "1       38\n",
      "2        8\n",
      "6        6\n",
      "3        3\n",
      "7        3\n",
      "9        3\n",
      "5        2\n",
      "7,8      2\n",
      "4        2\n",
      "4,8      1\n",
      "2,9      1\n",
      "1,4      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "id                  0\n",
      "text                0\n",
      "similarity_score    0\n",
      "label               0\n",
      "dtype: int64\n",
      "\n",
      "Text length statistics:\n",
      "count     177.000000\n",
      "mean      627.129944\n",
      "std      1004.791515\n",
      "min        11.000000\n",
      "25%        88.000000\n",
      "50%       303.000000\n",
      "75%       727.000000\n",
      "max      9549.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "STEP 2: TEXT PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "Sample preprocessed text:\n",
      "burn out among cybersecurity leaders at a frustrating high. in a world of high powered ai and evolving threat actors; cyber security leaders are facing significant amounts of burnout and stress. anyone experienced this as well? [\n",
      "\n",
      "================================================================================\n",
      "STEP 3: HANDLING MULTI-LABELS\n",
      "================================================================================\n",
      "\n",
      "Multi-label samples found: 5\n",
      "Multi-label samples: ['7,8' '4,8' '2,9' '1,4']\n",
      "\n",
      "Strategy: Taking first label from multi-label samples\n",
      "\n",
      "Final label distribution:\n",
      "label\n",
      "0    107\n",
      "1     39\n",
      "2      9\n",
      "3      3\n",
      "4      3\n",
      "5      2\n",
      "6      6\n",
      "7      5\n",
      "9      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAIN-TEST SPLIT\n",
      "================================================================================\n",
      "\n",
      "Training set size: 141\n",
      "Test set size: 36\n",
      "\n",
      "Training set label distribution:\n",
      "0    85\n",
      "1    31\n",
      "2     7\n",
      "3     3\n",
      "4     2\n",
      "5     2\n",
      "6     5\n",
      "7     4\n",
      "9     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set label distribution:\n",
      "0    22\n",
      "1     8\n",
      "2     2\n",
      "4     1\n",
      "6     1\n",
      "7     1\n",
      "9     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "STEP 5: FEATURE EXTRACTION (TF-IDF with Trigrams)\n",
      "================================================================================\n",
      "\n",
      "TF-IDF matrix shape (train): (141, 1000)\n",
      "TF-IDF matrix shape (test): (36, 1000)\n",
      "Number of features: 1000\n",
      "\n",
      "Sample trigram features:\n",
      "Example trigrams: ['approach soften fear', 'constantly second guessing', 'conversation don understand', 'conversation engineering feels', 'corporation close total', 'costs budgets approved', 'costs jack costs', 'cut conversation don', 'cybersecurity public health', 'department management team']\n",
      "\n",
      "================================================================================\n",
      "STEP 6: MODEL TRAINING (SVM)\n",
      "================================================================================\n",
      "\n",
      "Training SVM with linear kernel...\n",
      "\n",
      "Accuracy: 0.7222\n",
      "F1 Score (Weighted): 0.6889\n",
      "F1 Score (Macro): 0.3519\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        22\n",
      "           1       0.50      0.75      0.60         8\n",
      "           2       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         1\n",
      "           6       1.00      1.00      1.00         1\n",
      "           7       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.72        36\n",
      "   macro avg       0.34      0.37      0.35        36\n",
      "weighted avg       0.67      0.72      0.69        36\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 7: MODEL EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  2  1  0  0  0  0]\n",
      " [ 2  6  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0]\n",
      " [ 0  1  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0]]\n",
      "\n",
      "Confusion matrix saved to: confusion_matrix_svm_trigram.png\n",
      "\n",
      "================================================================================\n",
      "STEP 8: SAVING MODEL\n",
      "================================================================================\n",
      "\n",
      "Model saved to: svm_trigram_model.pkl\n",
      "Vectorizer saved to: vectorizer_trigram.pkl\n",
      "\n",
      "================================================================================\n",
      "STEP 9: PREDICTION ON NEW DATA\n",
      "================================================================================\n",
      "\n",
      "Text 1: I'm experiencing severe burnout in my cybersecurity role...\n",
      "Predicted Label: 1\n",
      "\n",
      "Text 2: Looking for recommendations on IT career development...\n",
      "Predicted Label: 0\n",
      "\n",
      "Text 3: Just got a new security analyst position, very excited!...\n",
      "Predicted Label: 0\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "Summary:\n",
      "- Model: SVM with linear kernel\n",
      "- Features: TF-IDF with trigrams (1-3 grams)\n",
      "- No hyperparameter tuning (kept simple)\n",
      "\n",
      "Files created:\n",
      "1. confusion_matrix_svm_trigram.png\n",
      "2. svm_trigram_model.pkl\n",
      "3. vectorizer_trigram.pkl\n"
     ]
    }
   ],
   "source": [
    "# MAIN PIPELINE\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the complete classification pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT CLASSIFICATION: SVM WITH TRIGRAMS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # File path\n",
    "    data_path = 'test_data_with_labels_BINARY.csv'\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    df = load_and_explore_data(data_path)\n",
    "    \n",
    "    # Step 2: Preprocess text\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: TEXT PREPROCESSING\")\n",
    "    print(\"=\" * 80)\n",
    "    df['text_processed'] = df['text'].apply(preprocess_text)\n",
    "    print(\"\\nSample preprocessed text:\")\n",
    "    print(df['text_processed'].iloc[0])\n",
    "    \n",
    "    # Step 3: Handle multi-labels\n",
    "    df = handle_multilabels(df, strategy='first')\n",
    "    \n",
    "    # Step 4: Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(df, test_size=0.2)\n",
    "    \n",
    "    # Preprocess train and test texts\n",
    "    X_train_processed = [preprocess_text(text) for text in X_train]\n",
    "    X_test_processed = [preprocess_text(text) for text in X_test]\n",
    "    \n",
    "    # Step 5: Feature extraction with trigrams\n",
    "    X_train_tfidf, X_test_tfidf, vectorizer = extract_features_tfidf_trigrams(\n",
    "        X_train_processed, X_test_processed, max_features=1000\n",
    "    )\n",
    "    \n",
    "    # Step 6: Train SVM model\n",
    "    model, y_pred = train_svm_model(X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "    \n",
    "    # Step 7: Evaluate model\n",
    "    evaluate_model(y_test, y_pred)\n",
    "    \n",
    "    # Step 8: Save model\n",
    "    save_model(\n",
    "        model, \n",
    "        vectorizer, \n",
    "        'svm_trigram_model.pkl',\n",
    "        'vectorizer_trigram.pkl'\n",
    "    )\n",
    "    \n",
    "    # Step 9: Test prediction on new data\n",
    "    sample_texts = [\n",
    "        \"I'm experiencing severe burnout in my cybersecurity role\",\n",
    "        \"Looking for recommendations on IT career development\",\n",
    "        \"Just got a new security analyst position, very excited!\"\n",
    "    ]\n",
    "    \n",
    "    predictions = predict_new_data(model, vectorizer, sample_texts)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(\"- Model: SVM with linear kernel\")\n",
    "    print(\"- Features: TF-IDF with trigrams (1-3 grams)\")\n",
    "    print(\"- No hyperparameter tuning (kept simple)\")\n",
    "    print(\"\\nFiles created:\")\n",
    "    print(\"1. confusion_matrix_svm_trigram.png\")\n",
    "    print(\"2. svm_trigram_model.pkl\")\n",
    "    print(\"3. vectorizer_trigram.pkl\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindscape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
