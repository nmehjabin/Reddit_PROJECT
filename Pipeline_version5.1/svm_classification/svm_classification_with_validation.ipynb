{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1JmXuBp84NnqgOETloL6p"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KisH1KicpOEt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1770762099355,
          "user_tz": 300,
          "elapsed": 19296,
          "user": {
            "displayName": "Nadia",
            "userId": "17436501635847667941"
          }
        },
        "outputId": "c3f85672-ebd3-4d3d-da5e-297bbd0709e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p '/content/drive/MyDrive/Colab Notebooks/svm_classification/outputs'"
      ],
      "metadata": {
        "id": "jUcYPA85qN1H",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1770762341517,
          "user_tz": 300,
          "elapsed": 944,
          "user": {
            "displayName": "Nadia",
            "userId": "17436501635847667941"
          }
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DVzPQg99LNav",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1770762361777,
          "user_tz": 300,
          "elapsed": 2234,
          "user": {
            "displayName": "Nadia",
            "userId": "17436501635847667941"
          }
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "SVM Classification with Trigrams\n",
        "=================================\n",
        "Train an SVM classifier on your labeled samples using trigram features.\n",
        "\n",
        "Difference from previous model:\n",
        "- Previous: TF-IDF with unigrams + bigrams (1,2)\n",
        "- This: TF-IDF with trigrams (3,3) or combined (1,3)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                             accuracy_score, f1_score, precision_score, recall_score)\n",
        "import re\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Update this to your Google Drive path\n",
        "DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/svm_classification/manual_label_batch1_updated.csv'\n",
        "\n",
        "# Update outputs to save in the same folder\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/Colab Notebooks/svm_classification/outputs/svm_trigram_model.pkl'\n",
        "VECTORIZER_SAVE_PATH = '/content/drive/MyDrive/Colab Notebooks/svm_classification/outputs/svm_trigram_vectorizer.pkl'\n",
        "CONFUSION_MATRIX_PATH = '/content/drive/MyDrive/Colab Notebooks/svm_classification/outputs/svm_trigram_confusion_matrix.png'\n",
        "\n",
        "# DATA_PATH = 'manual_label_batch1.xlsx'\n",
        "# MODEL_SAVE_PATH = 'outputs/svm_trigram_model.pkl'\n",
        "# VECTORIZER_SAVE_PATH = 'outputs/svm_trigram_vectorizer.pkl'\n",
        "# CONFUSION_MATRIX_PATH = 'outputs/svm_trigram_confusion_matrix.png'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigram options:\n",
        "# 'trigrams_only': (3, 3) - only 3-word sequences\n",
        "# 'unigrams_to_trigrams': (1, 3) - words, 2-word, and 3-word sequences\n",
        "NGRAM_TYPE = 'unigrams_to_trigrams'  # Change to 'trigrams_only' if you want only trigrams\n",
        "\n",
        "# Data split sizes\n",
        "VAL_SIZE = 0.15    # 15% for validation\n",
        "TEST_SIZE = 0.15   # 15% for test\n",
        "# This means: 70% train, 15% validation, 15% test\n",
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "YroZTZ6_Lxf9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1770762367216,
          "user_tz": 300,
          "elapsed": 12,
          "user": {
            "displayName": "Nadia",
            "userId": "17436501635847667941"
          }
        }
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean text data\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def handle_multilabels(df):\n",
        "    \"\"\"Take first label from multi-label samples\"\"\"\n",
        "    print(\"\\nHandling multi-label samples...\")\n",
        "    multi_label_mask = df['label'].astype(str).str.contains(',')\n",
        "    multi_label_count = multi_label_mask.sum()\n",
        "\n",
        "    if multi_label_count > 0:\n",
        "        print(f\"  Found {multi_label_count} multi-label samples\")\n",
        "        df['label'] = df['label'].astype(str).apply(lambda x: x.split(',')[0])\n",
        "\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    return df"
      ],
      "metadata": {
        "id": "EjZlOno7L9Hi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1770762369612,
          "user_tz": 300,
          "elapsed": 2,
          "user": {
            "displayName": "Nadia",
            "userId": "17436501635847667941"
          }
        }
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN TRAINING FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def train_svm_trigram():\n",
        "    \"\"\"Train SVM with trigram features\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SVM CLASSIFICATION WITH TRIGRAMS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 1. LOAD DATA\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[1/8] Loading data...\")\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f\"  Loaded {len(df)} samples\")\n",
        "    print(f\"  Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 2. PREPROCESS\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[2/8] Preprocessing text...\")\n",
        "    df['text_processed'] = df['text'].apply(preprocess_text)\n",
        "    df = handle_multilabels(df)\n",
        "\n",
        "    print(f\"\\n  Label distribution:\")\n",
        "    print(df['label'].value_counts().sort_index())\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 3. TRAIN-VALIDATION-TEST SPLIT\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(f\"\\n[3/8] Splitting data (val: {VAL_SIZE}, test: {TEST_SIZE})...\")\n",
        "    X = df['text_processed'].values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # First split: separate test set\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Second split: separate validation from remaining\n",
        "    val_size_adjusted = VAL_SIZE / (1 - TEST_SIZE)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_size_adjusted, \n",
        "        random_state=RANDOM_STATE, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    print(f\"  Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "    print(f\"  Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "    print(f\"  Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 4. FEATURE EXTRACTION WITH TRIGRAMS\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(f\"\\n[4/8] Extracting features...\")\n",
        "    print(f\"  N-gram type: {NGRAM_TYPE}\")\n",
        "\n",
        "    if NGRAM_TYPE == 'trigrams_only':\n",
        "        ngram_range = (3, 3)\n",
        "        print(f\"  Using: trigrams only (3-word sequences)\")\n",
        "    else:  # unigrams_to_trigrams\n",
        "        ngram_range = (1, 3)\n",
        "        print(f\"  Using: unigrams + bigrams + trigrams\")\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=1000,\n",
        "        min_df=2,\n",
        "        max_df=0.8,\n",
        "        ngram_range=ngram_range,\n",
        "        stop_words='english'\n",
        "    )\n",
        "\n",
        "    # Fit on training data only\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_val_tfidf = vectorizer.transform(X_val)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "    print(f\"   Feature matrix shape (train): {X_train_tfidf.shape}\")\n",
        "    print(f\"   Feature matrix shape (validation): {X_val_tfidf.shape}\")\n",
        "    print(f\"   Feature matrix shape (test): {X_test_tfidf.shape}\")\n",
        "    print(f\"   Number of features: {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "    # Show sample trigrams\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    trigram_features = [f for f in feature_names if len(f.split()) == 3]\n",
        "    if trigram_features:\n",
        "        print(f\"\\n  Sample trigrams extracted:\")\n",
        "        for i, tg in enumerate(trigram_features[:5], 1):\n",
        "            print(f\"    {i}. '{tg}'\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 5. TRAIN SVM MODEL\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(f\"\\n[5/8] Training SVM classifier...\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Try linear kernel\n",
        "    print(f\"\\n  Training SVM with linear kernel...\")\n",
        "    svm_linear = SVC(kernel='linear', random_state=RANDOM_STATE, \n",
        "                     class_weight='balanced', probability=True)\n",
        "    svm_linear.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_val_pred_linear = svm_linear.predict(X_val_tfidf)\n",
        "    val_acc_linear = accuracy_score(y_val, y_val_pred_linear)\n",
        "    val_f1_weighted_linear = f1_score(y_val, y_val_pred_linear, average='weighted')\n",
        "    val_f1_macro_linear = f1_score(y_val, y_val_pred_linear, average='macro')\n",
        "\n",
        "    results['linear'] = {\n",
        "        'model': svm_linear,\n",
        "        'val_accuracy': val_acc_linear,\n",
        "        'val_f1_weighted': val_f1_weighted_linear,\n",
        "        'val_f1_macro': val_f1_macro_linear,\n",
        "        'val_pred': y_val_pred_linear\n",
        "    }\n",
        "\n",
        "    print(f\"    Validation Accuracy: {val_acc_linear:.4f}\")\n",
        "    print(f\"    Validation F1 (Weighted): {val_f1_weighted_linear:.4f}\")\n",
        "    print(f\"    Validation F1 (Macro): {val_f1_macro_linear:.4f}\")\n",
        "\n",
        "    # Try RBF kernel\n",
        "    print(f\"\\n  Training SVM with rbf kernel...\")\n",
        "    svm_rbf = SVC(kernel='rbf', random_state=RANDOM_STATE, \n",
        "                  class_weight='balanced', probability=True)\n",
        "    svm_rbf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_val_pred_rbf = svm_rbf.predict(X_val_tfidf)\n",
        "    val_acc_rbf = accuracy_score(y_val, y_val_pred_rbf)\n",
        "    val_f1_weighted_rbf = f1_score(y_val, y_val_pred_rbf, average='weighted')\n",
        "    val_f1_macro_rbf = f1_score(y_val, y_val_pred_rbf, average='macro')\n",
        "\n",
        "    results['rbf'] = {\n",
        "        'model': svm_rbf,\n",
        "        'val_accuracy': val_acc_rbf,\n",
        "        'val_f1_weighted': val_f1_weighted_rbf,\n",
        "        'val_f1_macro': val_f1_macro_rbf,\n",
        "        'val_pred': y_val_pred_rbf\n",
        "    }\n",
        "\n",
        "    print(f\"    Validation Accuracy: {val_acc_rbf:.4f}\")\n",
        "    print(f\"    Validation F1 (Weighted): {val_f1_weighted_rbf:.4f}\")\n",
        "    print(f\"    Validation F1 (Macro): {val_f1_macro_rbf:.4f}\")\n",
        "\n",
        "    # Select best model based on validation F1\n",
        "    best_kernel = 'linear' if val_f1_weighted_linear >= val_f1_weighted_rbf else 'rbf'\n",
        "    best_model = results[best_kernel]['model']\n",
        "    best_val_pred = results[best_kernel]['val_pred']\n",
        "\n",
        "    print(f\"\\n   Best kernel (based on validation): {best_kernel}\")\n",
        "    print(f\"   Best validation F1 score: {results[best_kernel]['val_f1_weighted']:.4f}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 6. EVALUATE ON VALIDATION SET\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(f\"\\n[6/8] Validation Set Performance...\")\n",
        "    print(\"\\n  Classification Report (Validation):\")\n",
        "    print(classification_report(y_val, best_val_pred))\n",
        "\n",
        "    print(\"\\n  Confusion Matrix (Validation):\")\n",
        "    cm_val = confusion_matrix(y_val, best_val_pred)\n",
        "    print(cm_val)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 7. FINAL EVALUATION ON TEST SET\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(f\"\\n[7/8] Test Set Performance (Final Evaluation)...\")\n",
        "    y_test_pred = best_model.predict(X_test_tfidf)\n",
        "    \n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "    test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
        "    test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "    print(f\"\\n  Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"  Test F1 (Weighted): {test_f1_weighted:.4f}\")\n",
        "    print(f\"  Test F1 (Macro): {test_f1_macro:.4f}\")\n",
        "\n",
        "    print(\"\\n  Classification Report (Test):\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    print(\"\\n  Confusion Matrix (Test):\")\n",
        "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "    print(cm_test)\n",
        "\n",
        "    # Plot confusion matrices side by side\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "    axes[0].set_title('Validation Set Confusion Matrix', fontweight='bold')\n",
        "    axes[0].set_ylabel('True Label')\n",
        "    axes[0].set_xlabel('Predicted Label')\n",
        "    \n",
        "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
        "    axes[1].set_title('Test Set Confusion Matrix (Final)', fontweight='bold')\n",
        "    axes[1].set_ylabel('True Label')\n",
        "    axes[1].set_xlabel('Predicted Label')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(CONFUSION_MATRIX_PATH, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"\\n   Confusion matrices saved: {CONFUSION_MATRIX_PATH}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 8. SAVE MODEL\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(f\"\\n[8/8] Saving model...\")\n",
        "    with open(MODEL_SAVE_PATH, 'wb') as f:\n",
        "        pickle.dump(best_model, f)\n",
        "    print(f\"   Model saved: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    with open(VECTORIZER_SAVE_PATH, 'wb') as f:\n",
        "        pickle.dump(vectorizer, f)\n",
        "    print(f\"  Vectorizer saved: {VECTORIZER_SAVE_PATH}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # SUMMARY\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n Results Summary:\")\n",
        "    print(f\"  Model: SVM ({best_kernel} kernel)\")\n",
        "    print(f\"  Features: {NGRAM_TYPE}\")\n",
        "    print(f\"  Data split: {len(X_train)} train / {len(X_val)} val / {len(X_test)} test\")\n",
        "    print(f\"\\n  Validation Performance:\")\n",
        "    print(f\"    Accuracy: {results[best_kernel]['val_accuracy']*100:.2f}%\")\n",
        "    print(f\"    F1 (Weighted): {results[best_kernel]['val_f1_weighted']:.4f}\")\n",
        "    print(f\"    F1 (Macro): {results[best_kernel]['val_f1_macro']:.4f}\")\n",
        "    print(f\"\\n  Test Performance (Final):\")\n",
        "    print(f\"    Accuracy: {test_acc*100:.2f}%\")\n",
        "    print(f\"    F1 (Weighted): {test_f1_weighted:.4f}\")\n",
        "    print(f\"    F1 (Macro): {test_f1_macro:.4f}\")\n",
        "\n",
        "    print(\"\\n Files Created:\")\n",
        "    print(f\"  1. {MODEL_SAVE_PATH}\")\n",
        "    print(f\"  2. {VECTORIZER_SAVE_PATH}\")\n",
        "    print(f\"  3. {CONFUSION_MATRIX_PATH}\")\n",
        "\n",
        "    # Store results for return\n",
        "    results['best_kernel'] = best_kernel\n",
        "    results['test_accuracy'] = test_acc\n",
        "    results['test_f1_weighted'] = test_f1_weighted\n",
        "    results['test_f1_macro'] = test_f1_macro\n",
        "    results['y_val'] = y_val\n",
        "    results['y_val_pred'] = best_val_pred\n",
        "    results['y_test'] = y_test\n",
        "    results['y_test_pred'] = y_test_pred\n",
        "\n",
        "    return best_model, vectorizer, results\n"
      ],
      "metadata": {
        "id": "6wzRHntOMCpX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1770762416274,
          "user_tz": 300,
          "elapsed": 70,
          "user": {
            "displayName": "Nadia",
            "userId": "17436501635847667941"
          }
        }
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction Function\n",
        "def test_predictions():\n",
        "    \"\"\"Test the trained model on sample texts\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TESTING PREDICTIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load model\n",
        "    with open(MODEL_SAVE_PATH, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(VECTORIZER_SAVE_PATH, 'rb') as f:\n",
        "        vectorizer = pickle.load(f)\n",
        "\n",
        "    # Sample texts\n",
        "    sample_texts = [\n",
        "        \"I'm experiencing severe burnout in my cybersecurity role\",\n",
        "        \"Looking for career advice in IT security\",\n",
        "        \"Just got promoted to senior security analyst\",\n",
        "        \"Dealing with stress and anxiety at work\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for i, text in enumerate(sample_texts, 1):\n",
        "        # Preprocess\n",
        "        processed = preprocess_text(text)\n",
        "\n",
        "        # Vectorize\n",
        "        vectorized = vectorizer.transform([processed])\n",
        "\n",
        "        # Predict\n",
        "        prediction = model.predict(vectorized)[0]\n",
        "        probability = model.predict_proba(vectorized)\n",
        "        confidence = probability.max()\n",
        "\n",
        "        print(f\"\\n{i}. Text: {text[:60]}...\")\n",
        "        print(f\"   Predicted Label: {prediction}\")\n",
        "        print(f\"   Confidence: {confidence:.2%}\")"
      ],
      "metadata": {
        "id": "BuOgzAXWN_gq",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1770762420978,
          "user_tz": 300,
          "elapsed": 2,
          "user": {
            "displayName": "Nadia",
            "userId": "17436501635847667941"
          }
        }
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Train model\n",
        "    model, vectorizer, results = train_svm_trigram()\n",
        "\n",
        "    # Test predictions\n",
        "    print(\"\\n\")\n",
        "    test_predictions()\n",
        "\n",
        "    # Optional: Uncomment to run hyperparameter tuning\n",
        "    # print(\"\\n\")\n",
        "    # print(\"Would you like to run hyperparameter tuning? (takes 5-10 minutes)\")\n",
        "    # response = input(\"Run tuning? (y/n): \")\n",
        "    # if response.lower() == 'y':\n",
        "    #     # Load data again\n",
        "    #     df = pd.read_excel(DATA_PATH)\n",
        "    #     df['text_processed'] = df['text'].apply(preprocess_text)\n",
        "    #     df = handle_multilabels(df)\n",
        "    #     X = df['text_processed'].values\n",
        "    #     y = df['label'].values\n",
        "    #     X_train, X_test, y_train, y_test = train_test_split(\n",
        "    #         X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        "    #     )\n",
        "    #     # Vectorize\n",
        "    #     vectorizer = TfidfVectorizer(\n",
        "    #         max_features=1000,\n",
        "    #         min_df=2,\n",
        "    #         max_df=0.8,\n",
        "    #         ngram_range=(1, 3),\n",
        "    #         stop_words='english'\n",
        "    #     )\n",
        "    #     X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    #     # Tune\n",
        "    #     tuned_model = tune_svm_hyperparameters(X_train_tfidf, y_train)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ALL DONE! \")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "XwiOR5eQOLoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1770762425025,
          "user_tz": 300,
          "elapsed": 1028,
          "user": {
            "displayName": "Nadia",
            "userId": "17436501635847667941"
          }
        },
        "outputId": "ce08a80d-5c9c-41fe-c11e-5cb200217508"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SVM CLASSIFICATION WITH TRIGRAMS\n",
            "================================================================================\n",
            "\n",
            "[1/7] Loading data...\n",
            "  Loaded 188 samples\n",
            "  Columns: ['id', 'text', 'similarity_score', 'label']\n",
            "\n",
            "[2/7] Preprocessing text...\n",
            "\n",
            "Handling multi-label samples...\n",
            "\n",
            "  Label distribution:\n",
            "label\n",
            "0    139\n",
            "1     49\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[3/7] Splitting data (test size: 0.2)...\n",
            "  Training set: 150 samples\n",
            "  Test set: 38 samples\n",
            "\n",
            "[4/7] Extracting features...\n",
            "  N-gram type: unigrams_to_trigrams\n",
            "  Using: unigrams + bigrams + trigrams\n",
            "   Feature matrix shape (train): (150, 1000)\n",
            "   Feature matrix shape (test): (38, 1000)\n",
            "   Number of features: 1000\n",
            "\n",
            "  Sample trigrams extracted:\n",
            "    1. 'feel like im'\n",
            "    2. 'just don know'\n",
            "    3. 'learning new things'\n",
            "    4. 'like cybersecurity job'\n",
            "    5. 'looks like cybersecurity'\n",
            "\n",
            "[5/7] Training SVM classifier...\n",
            "\n",
            "  Training SVM with linear kernel...\n",
            "    Accuracy: 0.8947\n",
            "    F1 (Weighted): 0.8976\n",
            "    F1 (Macro): 0.8721\n",
            "\n",
            "  Training SVM with rbf kernel...\n",
            "    Accuracy: 0.8684\n",
            "    F1 (Weighted): 0.8519\n",
            "    F1 (Macro): 0.7923\n",
            "\n",
            "   Best kernel: linear\n",
            "   Best F1 score: 0.8976\n",
            "\n",
            "[6/7] Evaluating best model...\n",
            "\n",
            "  Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.89      0.93        28\n",
            "           1       0.75      0.90      0.82        10\n",
            "\n",
            "    accuracy                           0.89        38\n",
            "   macro avg       0.86      0.90      0.87        38\n",
            "weighted avg       0.91      0.89      0.90        38\n",
            "\n",
            "\n",
            "  Confusion Matrix:\n",
            "[[25  3]\n",
            " [ 1  9]]\n",
            "\n",
            "   Confusion matrix saved: /content/drive/MyDrive/Colab Notebooks/svm_classification/outputs/svm_trigram_confusion_matrix.png\n",
            "\n",
            "[7/7] Saving model...\n",
            "   Model saved: /content/drive/MyDrive/Colab Notebooks/svm_classification/outputs/svm_trigram_model.pkl\n",
            "  Vectorizer saved: /content/drive/MyDrive/Colab Notebooks/svm_classification/outputs/svm_trigram_vectorizer.pkl\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE!\n",
            "================================================================================\n",
            "\n",
            " Results Summary:\n",
            "  Model: SVM (linear kernel)\n",
            "  Features: unigrams_to_trigrams\n",
            "  Accuracy: 89.47%\n",
            "  F1 Score (Weighted): 0.8976\n",
            "  F1 Score (Macro): 0.8721\n",
            "\n",
            " Files Created:\n",
            "  1. /content/drive/MyDrive/Colab Notebooks/svm_classification/outputs/svm_trigram_model.pkl\n",
            "  2. /content/drive/MyDrive/Colab Notebooks/svm_classification/outputs/svm_trigram_vectorizer.pkl\n",
            "  3. /content/drive/MyDrive/Colab Notebooks/svm_classification/outputs/svm_trigram_confusion_matrix.png\n",
            "\n",
            " Comparison with Previous Model:\n",
            "  Previous: Logistic Regression with unigrams+bigrams \u2192 66.67% accuracy\n",
            "  Current:  SVM with unigrams_to_trigrams \u2192 89.47% accuracy\n",
            "   Improvement with trigrams!\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TESTING PREDICTIONS\n",
            "================================================================================\n",
            "\n",
            "Sample Predictions:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Text: I'm experiencing severe burnout in my cybersecurity role...\n",
            "   Predicted Label: 1\n",
            "   Confidence: 86.95%\n",
            "\n",
            "2. Text: Looking for career advice in IT security...\n",
            "   Predicted Label: 1\n",
            "   Confidence: 60.53%\n",
            "\n",
            "3. Text: Just got promoted to senior security analyst...\n",
            "   Predicted Label: 0\n",
            "   Confidence: 53.71%\n",
            "\n",
            "4. Text: Dealing with stress and anxiety at work...\n",
            "   Predicted Label: 1\n",
            "   Confidence: 50.00%\n",
            "\n",
            "================================================================================\n",
            "ALL DONE! \n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}