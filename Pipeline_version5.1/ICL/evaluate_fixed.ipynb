{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_predictions(results_df, ground_truth_df, label_column='label'):\n",
    "    \"\"\"Evaluate predictions against ground truth labels\"\"\"\n",
    "    \n",
    "    # FIXED: Check lengths first\n",
    "    print(f\"\\nLength check:\")\n",
    "    print(f\"Results DF: {len(results_df)}\")\n",
    "    print(f\"Ground truth DF: {len(ground_truth_df)}\")\n",
    "    \n",
    "    # Option 1: Merge on text column if available\n",
    "    if 'text' in results_df.columns and 'text' in ground_truth_df.columns:\n",
    "        print(\"Merging on 'text' column...\")\n",
    "        merged = results_df.merge(\n",
    "            ground_truth_df[['text', label_column]], \n",
    "            on='text', \n",
    "            how='inner',\n",
    "            suffixes=('', '_ground_truth')\n",
    "        )\n",
    "        merged = merged.rename(columns={label_column: 'true_label'})\n",
    "        \n",
    "    # Option 2: If there's an ID column\n",
    "    elif 'id' in results_df.columns and 'id' in ground_truth_df.columns:\n",
    "        print(\"Merging on 'id' column...\")\n",
    "        merged = results_df.merge(\n",
    "            ground_truth_df[['id', label_column]], \n",
    "            on='id', \n",
    "            how='inner'\n",
    "        )\n",
    "        merged = merged.rename(columns={label_column: 'true_label'})\n",
    "        \n",
    "    # Option 3: If lengths match and you're sure they're aligned\n",
    "    elif len(results_df) == len(ground_truth_df):\n",
    "        print(\"Lengths match - using direct assignment...\")\n",
    "        merged = results_df.copy()\n",
    "        merged['true_label'] = ground_truth_df[label_column].values\n",
    "        \n",
    "    # Option 4: Truncate to minimum length (use with caution!)\n",
    "    else:\n",
    "        print(f\"WARNING: Length mismatch! Truncating to shorter length...\")\n",
    "        min_len = min(len(results_df), len(ground_truth_df))\n",
    "        merged = results_df.head(min_len).copy()\n",
    "        merged['true_label'] = ground_truth_df.head(min_len)[label_column].values\n",
    "        print(f\"Using only first {min_len} samples\")\n",
    "    \n",
    "    print(f\"Merged dataset: {len(merged)} samples\")\n",
    "    \n",
    "    # DEBUG: Print what we're working with\n",
    "    print(\"\\nDEBUG INFO:\")\n",
    "    print(f\"Predicted label sample: {merged['predicted_label'].head()}\")\n",
    "    print(f\"Predicted label unique values: {merged['predicted_label'].unique()}\")\n",
    "    print(f\"True label sample: {merged['true_label'].head()}\")\n",
    "    print(f\"True label unique values: {merged['true_label'].unique()}\")\n",
    "    \n",
    "    # Ensure both are numeric - CRITICAL FIX\n",
    "    merged['predicted_numeric'] = pd.to_numeric(merged['predicted_label'], errors='coerce')\n",
    "    merged['true_label_numeric'] = pd.to_numeric(merged['true_label'], errors='coerce')\n",
    "    \n",
    "    # Check for NaN values BEFORE filtering\n",
    "    print(f\"\\nNaN in predictions: {merged['predicted_numeric'].isna().sum()}\")\n",
    "    print(f\"NaN in true labels: {merged['true_label_numeric'].isna().sum()}\")\n",
    "    \n",
    "    # Filter out errors (NaN from parsing failures) - MUST filter BOTH columns\n",
    "    valid_mask = merged['predicted_numeric'].notna() & merged['true_label_numeric'].notna()\n",
    "    valid_predictions = merged[valid_mask].copy()\n",
    "    \n",
    "    # NOW it's safe to convert to int\n",
    "    valid_predictions['predicted_numeric'] = valid_predictions['predicted_numeric'].astype(int)\n",
    "    valid_predictions['true_label_numeric'] = valid_predictions['true_label_numeric'].astype(int)\n",
    "    \n",
    "    error_count = len(merged) - len(valid_predictions)\n",
    "    \n",
    "    if error_count > 0:\n",
    "        print(f\"  Warning: {error_count} predictions had parsing errors and were excluded\")\n",
    "    \n",
    "    # Get arrays for sklearn\n",
    "    y_true = valid_predictions['true_label_numeric'].values\n",
    "    y_pred = valid_predictions['predicted_numeric'].values\n",
    "    \n",
    "    print(f\"\\nFinal arrays for evaluation:\")\n",
    "    print(f\"y_true: {y_true[:10]}\")\n",
    "    print(f\"y_pred: {y_pred[:10]}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "        'total_samples': len(merged),\n",
    "        'valid_predictions': len(valid_predictions),\n",
    "        'parsing_errors': error_count,\n",
    "        'true_positives': int(((y_pred == 1) & (y_true == 1)).sum()),\n",
    "        'true_negatives': int(((y_pred == 0) & (y_true == 0)).sum()),\n",
    "        'false_positives': int(((y_pred == 1) & (y_true == 0)).sum()),\n",
    "        'false_negatives': int(((y_pred == 0) & (y_true == 1)).sum()),\n",
    "    }\n",
    "    \n",
    "    return metrics, valid_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db6cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics, approach_name):\n",
    "    \"\"\"Pretty print evaluation metrics\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION RESULTS: {approach_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\n Overall Performance:\")\n",
    "    print(f\"   Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "    print(f\"   Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"   F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\ Sample Counts:\")\n",
    "    print(f\"   Total samples:        {metrics['total_samples']}\")\n",
    "    print(f\"   Valid predictions:    {metrics['valid_predictions']}\")\n",
    "    print(f\"   Parsing errors:       {metrics['parsing_errors']}\")\n",
    "    \n",
    "    print(f\"\\n Confusion Matrix Breakdown:\")\n",
    "    print(f\"   True Positives (TP):  {metrics['true_positives']}  (Correctly identified burnout)\")\n",
    "    print(f\"   True Negatives (TN):  {metrics['true_negatives']}  (Correctly identified no burnout)\")\n",
    "    print(f\"   False Positives (FP): {metrics['false_positives']}  (Incorrectly labeled as burnout)\")\n",
    "    print(f\"   False Negatives (FN): {metrics['false_negatives']}  (Missed burnout cases)\")\n",
    "    \n",
    "    print(f\"\\n Confusion Matrix:\")\n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"                  Predicted No (0)  Predicted Yes (1)\")\n",
    "    print(f\"   Actual No (0):        {cm[0][0]:4d}            {cm[0][1]:4d}\")\n",
    "    print(f\"   Actual Yes (1):       {cm[1][0]:4d}            {cm[1][1]:4d}\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4804addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(metrics, approach_name, save_path=None):\n",
    "    \"\"\"Plot confusion matrix as a heatmap\"\"\"\n",
    "    cm = metrics['confusion_matrix']\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No Burnout (0)', 'Burnout (1)'],\n",
    "                yticklabels=['No Burnout (0)', 'Burnout (1)'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title(f'Confusion Matrix: {approach_name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\" Confusion matrix saved: {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b3c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(valid_predictions, text_column='text'):\n",
    "    \"\"\"Analyze misclassified examples\"\"\"\n",
    "    \n",
    "    # Find misclassifications\n",
    "    errors = valid_predictions[valid_predictions['true_label_numeric'] != valid_predictions['predicted_numeric']].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ERROR ANALYSIS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total misclassifications: {len(errors)}\")\n",
    "    \n",
    "    if len(errors) == 0:\n",
    "        print(\" Perfect classification! No errors found.\")\n",
    "        return errors\n",
    "    \n",
    "    # False Positives (predicted burnout but actually not)\n",
    "    false_positives = errors[errors['predicted_numeric'] == 1]\n",
    "    print(f\"\\n False Positives: {len(false_positives)} (Predicted burnout, actually not)\")\n",
    "    \n",
    "    # False Negatives (predicted not burnout but actually is)\n",
    "    false_negatives = errors[errors['predicted_numeric'] == 0]\n",
    "    print(f\" False Negatives: {len(false_negatives)} (Missed burnout cases)\")\n",
    "    \n",
    "    # Show a few examples\n",
    "    print(f\"\\n Sample False Positives (first 3):\")\n",
    "    for idx, row in false_positives.head(3).iterrows():\n",
    "        print(f\"\\n   Text: {row[text_column][:200]}...\")\n",
    "        print(f\"   True: No Burnout (0) | Predicted: Burnout (1)\")\n",
    "        print(f\"   Model output: {row['raw_output']}\")\n",
    "    \n",
    "    print(f\"\\n Sample False Negatives (first 3):\")\n",
    "    for idx, row in false_negatives.head(3).iterrows():\n",
    "        print(f\"\\n   Text: {row[text_column][:200]}...\")\n",
    "        print(f\"   True: Burnout (1) | Predicted: No Burnout (0)\")\n",
    "        print(f\"   Model output: {row['raw_output']}\")\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b764d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth labels...\n",
      " Ground truth loaded: 188 samples\n",
      "\n",
      "Loading prediction results...\n",
      " Results loaded: 168 samples\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (188) does not match length of index (168)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Results loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m metrics, valid_predictions = \u001b[43mevaluate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLABEL_COLUMN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m approach_name = results_df[\u001b[33m'\u001b[39m\u001b[33mapproach\u001b[39m\u001b[33m'\u001b[39m].iloc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mapproach\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m results_df.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mSingle Approach\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Print metrics\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mevaluate_predictions\u001b[39m\u001b[34m(results_df, ground_truth_df, label_column)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Merge predictions with ground truth\u001b[39;00m\n\u001b[32m     11\u001b[39m merged = results_df.copy()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mmerged\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrue_label\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = ground_truth_df[label_column].values\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# DEBUG: Print what we're working with\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDEBUG INFO:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/site-packages/pandas/core/frame.py:4322\u001b[39m, in \u001b[36mDataFrame.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4319\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_array([key], value)\n\u001b[32m   4320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4321\u001b[39m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4322\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/site-packages/pandas/core/frame.py:4535\u001b[39m, in \u001b[36mDataFrame._set_item\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4525\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4526\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4527\u001b[39m \u001b[33;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[32m   4528\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4533\u001b[39m \u001b[33;03m    ensure homogeneity.\u001b[39;00m\n\u001b[32m   4534\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4535\u001b[39m     value, refs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4537\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4538\u001b[39m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns\n\u001b[32m   4539\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m value.ndim == \u001b[32m1\u001b[39m\n\u001b[32m   4540\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value.dtype, ExtensionDtype)\n\u001b[32m   4541\u001b[39m     ):\n\u001b[32m   4542\u001b[39m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[32m   4543\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.is_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/site-packages/pandas/core/frame.py:5288\u001b[39m, in \u001b[36mDataFrame._sanitize_column\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   5285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m.index)\n\u001b[32m   5287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[32m-> \u001b[39m\u001b[32m5288\u001b[39m     \u001b[43mcom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5289\u001b[39m arr = sanitize_array(value, \u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   5290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   5291\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[32m   5292\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m value.dtype == \u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5295\u001b[39m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[32m   5296\u001b[39m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/myenv/lib/python3.11/site-packages/pandas/core/common.py:573\u001b[39m, in \u001b[36mrequire_length_match\u001b[39m\u001b[34m(data, index)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[33;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) != \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    574\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLength of values \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    575\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdoes not match length of index \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Length of values (188) does not match length of index (168)"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN EVALUATION - UPDATE FILE PATHS HERE\n",
    "# ============================================================================\n",
    "\n",
    "TEXT_COLUMN = 'text'  \n",
    "LABEL_COLUMN = 'label' \n",
    "\n",
    "# CORRECTED FILE PATH - \n",
    "ground_truth_path = 'manual_label_batch1_updated.csv'\n",
    "# results_path = 'results_Zero-Shot_Claude-Sonnet.csv'\n",
    "results_path = 'results_Few-Shot_Claude-Sonnet.csv'\n",
    "\n",
    "# Load ground truth labels\n",
    "print(\"Loading ground truth labels...\")\n",
    "ground_truth = pd.read_csv(ground_truth_path)\n",
    "print(f\" Ground truth loaded: {len(ground_truth)} samples\")\n",
    "\n",
    "# Load prediction results\n",
    "print(\"\\nLoading prediction results...\")\n",
    "results_df = pd.read_csv(results_path)\n",
    "print(f\" Results loaded: {len(results_df)} samples\")\n",
    "\n",
    "# Evaluate\n",
    "metrics, valid_predictions = evaluate_predictions(results_df, ground_truth, LABEL_COLUMN)\n",
    "approach_name = results_df['approach'].iloc[0] if 'approach' in results_df.columns else 'Single Approach'\n",
    "\n",
    "# Print metrics\n",
    "print_metrics(metrics, approach_name)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(metrics, approach_name, save_path=f'confusion_matrix_{approach_name}.png')\n",
    "\n",
    "# Analyze errors\n",
    "errors = analyze_errors(valid_predictions, TEXT_COLUMN)\n",
    "\n",
    "# Save error analysis\n",
    "if len(errors) > 0:\n",
    "    errors.to_csv(f'error_analysis_{approach_name}.csv', index=False)\n",
    "    print(f\"\\n Error analysis saved: error_analysis_{approach_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FIXING FEW-SHOT RESULTS\n",
      "======================================================================\n",
      "\n",
      "Before fix:\n",
      "predicted_label\n",
      "-1      1\n",
      " 0    115\n",
      " 1     51\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Found 1 prediction(s) with value -1\n",
      "\n",
      "Error row details:\n",
      "  Text: Dealing with feeling stuck in the security field\n",
      "### Opening Statement\n",
      "\n",
      "In argument theory, there's a concept of a false dichotomy. Within a false dic...\n",
      "  Raw output: Looking at this input text, I need to determine if the poster is discussing their own work-related burnout/stress in the past or present.\n",
      "\n",
      "The poster is writing an advice/guidance post for others who might be feeling \"stuck in the security field.\" While they mention burnout as a concept and acknowledge that workplaces aren't perfect (\"sunshine and rainbows\"), they are not discussing their own personal experience with burnout or work-related stress.\n",
      "\n",
      "Key indicators this is NOT\n",
      "  Current predicted_label: -1\n",
      "\n",
      "After reparsing:\n",
      "  Fixed -1 value to: 0\n",
      "\n",
      "After fix:\n",
      "predicted_label\n",
      "0    116\n",
      "1     51\n",
      "Name: count, dtype: int64\n",
      "\n",
      " All -1 values fixed!\n",
      "\n",
      "======================================================================\n",
      "SAVED\n",
      "======================================================================\n",
      "Fixed results saved to:\n",
      "  results_Few-Shot_Claude-Sonnet_FIXED.csv\n",
      "\n",
      "You can now use this file for evaluation!\n",
      "\n",
      "Do you want to overwrite the original file? (y/n)\n",
      "If yes, manually rename results_Few-Shot_Claude-Sonnet_FIXED.csv\n",
      "to results_Few-Shot_Claude-Sonnet.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # ============================================================================\n",
    "# # FIX THE -1 VALUE IN FEW-SHOT RESULTS\n",
    "# # ============================================================================\n",
    "\n",
    "# fewshot_path = 'results_Few-Shot_Claude-Sonnet.csv'\n",
    "\n",
    "# print(\"=\"*70)\n",
    "# print(\"FIXING FEW-SHOT RESULTS\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Load the results\n",
    "# results_df = pd.read_csv(fewshot_path)\n",
    "\n",
    "# print(f\"\\nBefore fix:\")\n",
    "# print(results_df['predicted_label'].value_counts().sort_index())\n",
    "\n",
    "# # Find the -1 prediction\n",
    "# error_row = results_df[results_df['predicted_label'] == -1]\n",
    "# print(f\"\\nFound {len(error_row)} prediction(s) with value -1\")\n",
    "\n",
    "# if len(error_row) > 0:\n",
    "#     print(\"\\nError row details:\")\n",
    "#     for idx, row in error_row.iterrows():\n",
    "#         print(f\"  Text: {row['text'][:150]}...\")\n",
    "#         print(f\"  Raw output: {row['raw_output']}\")\n",
    "#         print(f\"  Current predicted_label: {row['predicted_label']}\")\n",
    "    \n",
    "#     # Strategy: Look at the raw output and try to extract 0 or 1\n",
    "#     # If we can't find it, default to 0 (no burnout) as the conservative choice\n",
    "    \n",
    "#     def reparse_output(output):\n",
    "#         \"\"\"Try to extract 0 or 1 from the output\"\"\"\n",
    "#         output_str = str(output).strip().lower()\n",
    "        \n",
    "#         # Check if output contains reasoning about burnout\n",
    "#         burnout_keywords = ['burnout', 'stress', 'experiencing', 'discussing their own']\n",
    "#         no_burnout_keywords = ['not discussing', 'advice', 'guidance', 'hypothetical', 'others']\n",
    "        \n",
    "#         # Count keywords\n",
    "#         burnout_count = sum(1 for kw in burnout_keywords if kw in output_str)\n",
    "#         no_burnout_count = sum(1 for kw in no_burnout_keywords if kw in output_str)\n",
    "        \n",
    "#         # Look for explicit 0 or 1\n",
    "#         if \"1\" in output_str:\n",
    "#             return 1\n",
    "#         elif \"0\" in output_str:\n",
    "#             return 0\n",
    "#         # If no explicit answer but we can infer from keywords\n",
    "#         elif 'advice' in output_str or 'guidance' in output_str or 'others' in output_str:\n",
    "#             return 0  # Likely advising others, not personal burnout\n",
    "#         else:\n",
    "#             # Conservative default: no burnout\n",
    "#             return 0\n",
    "    \n",
    "#     # Apply the fix\n",
    "#     results_df.loc[results_df['predicted_label'] == -1, 'predicted_label'] = \\\n",
    "#         results_df.loc[results_df['predicted_label'] == -1, 'raw_output'].apply(reparse_output)\n",
    "    \n",
    "#     print(f\"\\nAfter reparsing:\")\n",
    "#     print(f\"  Fixed -1 value to: {results_df.loc[error_row.index[0], 'predicted_label']}\")\n",
    "\n",
    "# print(f\"\\nAfter fix:\")\n",
    "# print(results_df['predicted_label'].value_counts().sort_index())\n",
    "\n",
    "# # Verify no -1 values remain\n",
    "# if -1 in results_df['predicted_label'].values:\n",
    "#     print(\"\\n Warning: Still have -1 values!\")\n",
    "# else:\n",
    "#     print(\"\\n All -1 values fixed!\")\n",
    "\n",
    "# # Save the fixed file\n",
    "# output_path = fewshot_path.replace('.csv', '_FIXED.csv')\n",
    "# results_df.to_csv(output_path, index=False)\n",
    "\n",
    "# print(f\"\\n\" + \"=\"*70)\n",
    "# print(\"SAVED\")\n",
    "# print(\"=\"*70)\n",
    "# print(f\"Fixed results saved to:\")\n",
    "# print(f\"  {output_path}\")\n",
    "# print(f\"\\nYou can now use this file for evaluation!\")\n",
    "\n",
    "# # Also update the original file if you want\n",
    "# print(f\"\\nDo you want to overwrite the original file? (y/n)\")\n",
    "# print(f\"If yes, manually rename {output_path}\")\n",
    "# print(f\"to {fewshot_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
