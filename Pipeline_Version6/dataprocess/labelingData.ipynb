{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a86a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I have 3 datasets: \n",
    "1. manual_label_batch1.xlsx -> we are gonna convert all other tags label 2-9 as label 1 to put them into relevant catagory\n",
    "and save the file as csv file \"x\". this file has 177 data rows\n",
    "2. burnout/-search_results.csv -> check if this file contains same data as file \"x\", if so, then we want to merge the remaining \n",
    "data rows of this file to the \"x\" to label the remaning data. this file has 289 rows\n",
    "\n",
    "both files has different data frame header\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2f9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Load files ---\n",
    "df1 = pd.read_excel('manual_label_batch1.xlsx')\n",
    "df2 = pd.read_csv('burnout_search_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294ddde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution after remap:\n",
      "label\n",
      "0    107\n",
      "1     70\n",
      "Name: count, dtype: int64\n",
      "Total rows in df1: 177\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Remap labels 2-9 and compound labels (e.g. '7,8', '1,4') → 1, keep 0 as 0 ---\n",
    "def remap_label(val):\n",
    "    if str(val) == '0':\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "df1['label'] = df1['label'].apply(remap_label)\n",
    "\n",
    "print('Label distribution after remap:')\n",
    "print(df1['label'].value_counts().sort_index())\n",
    "print(f'Total rows in df1: {len(df1)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb85592f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overlapping rows (df2 already in df1): 177\n",
      "Remaining rows to label from df2:     112\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Check overlap between df1 and df2 by 'id' ---\n",
    "ids_in_df1 = set(df1['id'].astype(str))\n",
    "df2['id'] = df2['id'].astype(str)\n",
    "\n",
    "overlap   = df2[df2['id'].isin(ids_in_df1)]\n",
    "remaining = df2[~df2['id'].isin(ids_in_df1)]\n",
    "\n",
    "print(f'\\nOverlapping rows (df2 already in df1): {len(overlap)}')\n",
    "print(f'Remaining rows to label from df2:     {len(remaining)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d686f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Format remaining rows to match df1 schema ---\n",
    "remaining_formatted = pd.DataFrame({\n",
    "    'id':               remaining['id'],\n",
    "    'text':             remaining['text'],\n",
    "    'similarity_score': remaining['similarity_score'],\n",
    "    'label':            None   # unlabeled — awaiting annotation\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4948ef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final merged dataset shape: (289, 4)\n",
      "label\n",
      "None    112\n",
      "0       107\n",
      "1        70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Merge labeled df1 + unlabeled remaining rows ---\n",
    "merged = pd.concat([df1, remaining_formatted], ignore_index=True)\n",
    "\n",
    "print(f'\\nFinal merged dataset shape: {merged.shape}')\n",
    "print(merged['label'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33d5ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved to merged_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Save ---\n",
    "merged.to_csv('merged_dataset.csv', index=False)\n",
    "print('\\nSaved to merged_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33e4a94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded merged dataset shape: (289, 4)\n",
      "label\n",
      "0    173\n",
      "1    116\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#load the merged dataset to check\n",
    "merged_check = pd.read_csv('merged_dataset.csv')\n",
    "print(f'\\nLoaded merged dataset shape: {merged_check.shape}')\n",
    "print(merged_check['label'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac26fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final merged dataset shape after adding sleep posts: (309, 4)\n",
      "label\n",
      "0    173\n",
      "1    136\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved to final_merged_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r1/_8ps3x2x6nb6r77rz8dp80sh0000gn/T/ipykernel_84952/2284816369.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_merged = pd.concat([merged_check, sampled_sleep_formatted], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now I wanna add sleep tag in the dataset,\n",
    "i will randomly pick 20 posts from posts_processed.csv \n",
    "big data file and merge it to the merged dataset and label \n",
    "them as relevant\n",
    "\n",
    "'''\n",
    "df3 = pd.read_csv('posts_processed2.csv')\n",
    "# --- Step 6: Randomly sample 20 posts from df3 if keyword =sleep---\n",
    "sleep_posts = df3[df3['text'].str.contains('sleep', case=False, na=False)]\n",
    "\n",
    "\n",
    "sampled_sleep_posts = sleep_posts.sample(n=20, random_state=42)\n",
    "# --- Step 7: Format sampled sleep posts to match merged dataset schema ---\n",
    "sampled_sleep_formatted = pd.DataFrame({\n",
    "    'id':               sampled_sleep_posts['id'],\n",
    "    'text':             sampled_sleep_posts['text'],\n",
    "    'similarity_score': None,  # No similarity score for these new posts\n",
    "    'label':            1     # Label as relevant\n",
    "})\n",
    "\n",
    "\n",
    "# --- Step 8: Merge sampled sleep posts into the merged dataset ---\n",
    "final_merged = pd.concat([merged_check, sampled_sleep_formatted], ignore_index=True)\n",
    "print(f'\\nFinal merged dataset shape after adding sleep posts: {final_merged.shape}')\n",
    "print(final_merged['label'].value_counts(dropna=False))\n",
    "# --- Step 9: Save the final merged dataset ---\n",
    "final_merged.to_csv('final_merged_dataset.csv', index=False)\n",
    "print('\\nSaved to final_merged_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindscape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
